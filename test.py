#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç®€å•é‡åŒ–æµ‹è¯• - ç”¨ Qwen2-0.5B æ¨¡å‹
RTX 3050 å¯ä»¥è½»æ¾è·‘é€š
"""

import os
import sys
import torch
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig


def load_model_fp16():
    """åŠ è½½ FP16 æ¨¡å‹"""
    print("\n" + "=" * 70)
    print("ğŸ“¦ åŠ è½½ FP16 æ¨¡å‹")
    print("=" * 70 + "\n")

    print("[1/2] åŠ è½½ Tokenizer...     ", end="", flush=True)
    sys.stdout.flush()

    tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen2-0.5B-Instruct",
        trust_remote_code=True
    )
    print("âœ…\n")

    print("[2/2] åŠ è½½æ¨¡å‹ (FP16)...    ", end="", flush=True)
    sys.stdout.flush()

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2-0.5B-Instruct",
        torch_dtype=torch.float16,
        device_map="cuda",
        trust_remote_code=True
    )

    model.eval()
    print("âœ…\n")

    # æ˜¾å­˜ç»Ÿè®¡
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    allocated_memory = torch.cuda.memory_allocated() / 1e9

    print("ğŸ“Š æ˜¾å­˜ç»Ÿè®¡:")
    print(f"   æ€»æ˜¾å­˜: {total_memory:.2f} GB")
    print(f"   å·²ç”¨: {allocated_memory:.2f} GB")
    print(f"   ä½¿ç”¨ç‡: {(allocated_memory / total_memory * 100):.1f}%\n")

    return model, tokenizer


def load_model_4bit():
    """åŠ è½½ 4-bit é‡åŒ–æ¨¡å‹"""
    print("\n" + "=" * 70)
    print("ğŸ“¦ åŠ è½½ 4-bit é‡åŒ–æ¨¡å‹")
    print("=" * 70 + "\n")

    print("[1/2] åŠ è½½ Tokenizer...     ", end="", flush=True)
    sys.stdout.flush()

    tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen2-0.5B-Instruct",
        trust_remote_code=True
    )
    print("âœ…\n")

    print("[2/2] åŠ è½½æ¨¡å‹ (4-bit)...      ", end="", flush=True)
    sys.stdout.flush()

    # 4-bit é‡åŒ–é…ç½®
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
    )

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2-0.5B-Instruct",
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True
    )

    model.eval()
    print("âœ…\n")

    # æ˜¾å­˜ç»Ÿè®¡
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    allocated_memory = torch.cuda.memory_allocated() / 1e9

    print("ğŸ“Š æ˜¾å­˜ç»Ÿè®¡:")
    print(f"   æ€»æ˜¾å­˜: {total_memory:.2f} GB")
    print(f"   å·²ç”¨: {allocated_memory:.2f} GB")
    print(f"   ä½¿ç”¨ç‡: {(allocated_memory / total_memory * 100):.1f}%\n")

    return model, tokenizer


def test_inference(model, tokenizer, text="ä½ å¥½ï¼Œå‘Šè¯‰æˆ‘ä½ æ˜¯è°"):
    """æµ‹è¯•æ¨ç†"""
    print("=" * 70)
    print("ğŸ§ª æµ‹è¯•æ¨ç†")
    print("=" * 70 + "\n")

    print(f"ğŸ“ è¾“å…¥æ–‡æœ¬: {text}\n")
    print("â³ æ¨ç†ä¸­...      ", end="", flush=True)
    sys.stdout.flush()

    try:
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                top_p=0.95,
            )

        response = tokenizer.decode(output[0], skip_special_tokens=True)
        print("âœ…\n")

        print(f"ğŸ’¬ è¾“å‡ºæ–‡æœ¬:\n{response}\n")

        return True

    except Exception as e:
        print(f"âŒ")
        print(f"\nâŒ æ¨ç†å¤±è´¥: {str(e)}\n")
        return False


def compare_models():
    """å¯¹æ¯” FP16 å’Œ 4-bit"""
    print("\n\n" + "=" * 70)
    print("ğŸ“Š FP16 vs 4-bit å¯¹æ¯”")
    print("=" * 70 + "\n")

    # FP16
    print("1ï¸âƒ£  åŠ è½½ FP16 æ¨¡å‹...")
    torch.cuda.empty_cache()
    gc.collect()

    model_fp16, tokenizer_fp16 = load_model_fp16()
    fp16_memory = torch.cuda.memory_allocated() / 1e9

    # æ¨ç†æµ‹è¯•
    test_inference(model_fp16, tokenizer_fp16, "ä½ å¥½")

    # æ¸…ç†
    del model_fp16
    torch.cuda.empty_cache()
    gc.collect()

    # 4-bit
    print("\n2ï¸âƒ£  åŠ è½½ 4-bit é‡åŒ–æ¨¡å‹...")
    torch.cuda.empty_cache()
    gc.collect()

    model_4bit, tokenizer_4bit = load_model_4bit()
    bit4_memory = torch.cuda.memory_allocated() / 1e9

    # æ¨ç†æµ‹è¯•
    test_inference(model_4bit, tokenizer_4bit, "ä½ å¥½")

    # å¯¹æ¯”è¡¨æ ¼
    print("\n" + "=" * 70)
    print("ğŸ“Š å¯¹æ¯”ç»“æœ")
    print("=" * 70 + "\n")

    print(f"{'æ¨¡å¼':<15} {'æ˜¾å­˜å ç”¨':<15} {'èŠ‚çœæ¯”ä¾‹':<15}")
    print("-" * 45)
    print(f"{'FP16':<15} {fp16_memory:>12.2f} GB {'-':>13}")
    print(f"{'4-bit':<15} {bit4_memory:>12.2f} GB {(1 - bit4_memory / fp16_memory) * 100:>12.1f}%")
    print()


def main():
    """ä¸»å‡½æ•°"""

    import argparse

    parser = argparse.ArgumentParser(description="ç®€å•é‡åŒ–æµ‹è¯•")
    parser.add_argument(
        "--mode",
        type=str,
        default="compare",
        choices=["fp16", "4bit", "compare"],
        help="è¿è¡Œæ¨¡å¼"
    )

    args = parser.parse_args()

    if args.mode == "fp16":
        model, tokenizer = load_model_fp16()
        test_inference(model, tokenizer)

    elif args.mode == "4bit":
        model, tokenizer = load_model_4bit()
        test_inference(model, tokenizer)

    elif args.mode == "compare":
        compare_models()

    print("\nâœ… æµ‹è¯•å®Œæˆï¼\n")


if __name__ == '__main__':
    main()