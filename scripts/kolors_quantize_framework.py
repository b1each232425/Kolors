#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Kolors é‡åŒ–æ¡†æ¶ - ç¬¬1æ­¥ï¼šæ¨¡å‹åŠ è½½ + 4-bit é‡åŒ–ï¼ˆç¨³å®šç‰ˆï¼‰
"""

import os
import sys
import torch
import gc
from kolors.models.modeling_chatglm import ChatGLMModel
from kolors.models.tokenization_chatglm import ChatGLMTokenizer
from diffusers import UNet2DConditionModel, AutoencoderKL, EulerDiscreteScheduler

root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


def load_models(quantize_mode="fp16", device="cuda"):
    """
    åŠ è½½æ‰€æœ‰æ¨¡å‹çš„æ¡†æ¶å‡½æ•°

    Args:
        quantize_mode: "fp16" æˆ– "4bit"
        device: "cuda" æˆ– "cpu"

    Returns:
        dict: åŒ…å«æ‰€æœ‰åŠ è½½çš„æ¨¡å‹
    """

    ckpt_dir = f'{root_dir}/weights/Kolors'
    models = {}

    print("\n" + "=" * 70)
    print("ğŸ“¦ Kolors æ¨¡å‹åŠ è½½æ¡†æ¶")
    print("=" * 70)
    print(f"\nğŸ’¡ é…ç½®:")
    print(f"   é‡åŒ–æ¨¡å¼: {quantize_mode}")
    print(f"   è®¾å¤‡: {device}")
    print(f"   æ¨¡å‹è·¯å¾„: {ckpt_dir}\n")

    try:
        # ========== åŠ è½½æ–‡æœ¬ç¼–ç å™¨ï¼ˆæ”¯æŒ 4-bit é‡åŒ–ï¼‰==========
        print("[1/5] åŠ è½½æ–‡æœ¬ç¼–ç å™¨...     ", end="", flush=True)
        sys.stdout.flush()

        if quantize_mode == "4bit":
            print("\n   â†’ ä½¿ç”¨ 4-bit é‡åŒ–")
            sys.stdout.flush()

            from transformers import BitsAndBytesConfig
            import warnings
            warnings.filterwarnings("ignore")

            # 4-bit é‡åŒ–é…ç½®
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )

            print("   â†’ å¼€å§‹åŠ è½½æ¨¡å‹ï¼ˆå¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´ï¼‰...    ", end="", flush=True)
            sys.stdout.flush()

            text_encoder = ChatGLMModel.from_pretrained(
                f'{ckpt_dir}/text_encoder',
                quantization_config=quantization_config,
                device_map="sequential",  # æ”¹ç”¨ sequential è€Œä¸æ˜¯ auto
                low_cpu_mem_usage=True,
            )

            print("âœ…")
            print("   âœ… 4-bit é‡åŒ–æˆåŠŸ\n")

        else:  # fp16
            print("\n   â†’ ä½¿ç”¨ FP16 ç²¾åº¦")
            sys.stdout.flush()

            text_encoder = ChatGLMModel.from_pretrained(
                f'{ckpt_dir}/text_encoder',
                torch_dtype=torch.float16,
            ).half()

            if device == "cuda":
                text_encoder = text_encoder.to("cuda")

            print("   âœ… FP16 åŠ è½½æˆåŠŸ\n")

        text_encoder.eval()
        models['text_encoder'] = text_encoder
        print("âœ…\n")

        torch.cuda.empty_cache()
        gc.collect()

        # ========== åŠ è½½ Tokenizer ==========
        print("[2/5] åŠ è½½ Tokenizer...   ", end="", flush=True)
        sys.stdout.flush()

        tokenizer = ChatGLMTokenizer.from_pretrained(f'{ckpt_dir}/text_encoder')
        models['tokenizer'] = tokenizer
        print("âœ…\n")

        torch.cuda.empty_cache()
        gc.collect()

        # ========== åŠ è½½ VAE ==========
        print("[3/5] åŠ è½½ VAE...   ", end="", flush=True)
        sys.stdout.flush()

        vae = AutoencoderKL.from_pretrained(
            f"{ckpt_dir}/vae",
            revision=None,
            torch_dtype=torch.float16,
        ).half()

        if device == "cuda":
            vae = vae.to("cuda")

        vae.eval()
        models['vae'] = vae
        print("âœ…\n")

        torch.cuda.empty_cache()
        gc.collect()

        # ========== åŠ è½½ UNet ==========
        print("[4/5] åŠ è½½ UNet...    ", end="", flush=True)
        sys.stdout.flush()

        unet = UNet2DConditionModel.from_pretrained(
            f"{ckpt_dir}/unet",
            revision=None,
            torch_dtype=torch.float16,
        ).half()

        if device == "cuda":
            unet = unet.to("cuda")

        unet.eval()
        models['unet'] = unet
        print("âœ…\n")

        torch.cuda.empty_cache()
        gc.collect()

        # ========== åŠ è½½ Scheduler ==========
        print("[5/5] åŠ è½½ Scheduler...   ", end="", flush=True)
        sys.stdout.flush()

        scheduler = EulerDiscreteScheduler.from_pretrained(
            f"{ckpt_dir}/scheduler"
        )

        models['scheduler'] = scheduler
        print("âœ…\n")

        # ========== æ‰“å°åŠ è½½ä¿¡æ¯ ==========
        print("=" * 70)
        print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print("=" * 70)

        # æ˜¾å­˜ç»Ÿè®¡
        if device == "cuda":
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            allocated_memory = torch.cuda.memory_allocated() / 1e9
            peak_memory = torch.cuda.max_memory_allocated() / 1e9

            print(f"\nğŸ“Š æ˜¾å­˜ç»Ÿè®¡:")
            print(f"   æ€»æ˜¾å­˜: {total_memory:. 2f} GB")
            print(f"   å·²ç”¨: {allocated_memory:.2f} GB")
            print(f"   å³°å€¼: {peak_memory:. 2f} GB")

        print(f"\nğŸ“¦ åŠ è½½çš„æ¨¡å‹:")
        print(f"   âœ… text_encoder (æ–‡æœ¬ç¼–ç å™¨) - {quantize_mode}")
        print(f"   âœ… tokenizer (åˆ†è¯å™¨)")
        print(f"   âœ… vae (å˜åˆ†è‡ªç¼–ç å™¨) - FP16")
        print(f"   âœ… unet (æ‰©æ•£æ¨¡å‹) - FP16")
        print(f"   âœ… scheduler (é‡‡æ ·è°ƒåº¦å™¨)")
        print()

        return models

    except Exception as e:
        print(f"âŒ\n")
        print(f"âŒ åŠ è½½å¤±è´¥: {type(e).__name__}")
        print(f"   {str(e)}\n")

        import traceback
        traceback.print_exc()
        sys.exit(1)


def test_models(models):
    """
    æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸åŠ è½½

    Args:
        models: load_models() è¿”å›çš„æ¨¡å‹å­—å…¸
    """

    print("=" * 70)
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹")
    print("=" * 70 + "\n")

    try:
        # æµ‹è¯• text_encoder
        print("[1/3] æµ‹è¯• text_encoder...     ", end="", flush=True)
        sys.stdout.flush()

        tokenizer = models['tokenizer']
        text_encoder = models['text_encoder']

        test_text = "ä½ å¥½"
        inputs = tokenizer(test_text, return_tensors="pt")

        with torch.no_grad():
            device = next(text_encoder.parameters()).device
            input_ids = inputs.input_ids.to(device)
            output = text_encoder(input_ids)

        print("âœ…")
        print(f"   è¾“å…¥: '{test_text}'")
        output_shape = output[0].shape if isinstance(output, tuple) else output.shape
        print(f"   è¾“å‡ºå½¢çŠ¶: {output_shape}\n")

        # æµ‹è¯• VAE
        print("[2/3] æµ‹è¯• VAE...    ", end="", flush=True)
        sys.stdout.flush()

        vae = models['vae']

        # åˆ›å»ºå‡çš„å›¾åƒå¼ é‡
        fake_image = torch.randn(1, 4, 32, 32, dtype=torch.float16)
        if torch.cuda.is_available():
            fake_image = fake_image.to("cuda")

        with torch.no_grad():
            vae_output = vae.decode(fake_image).sample

        print("âœ…")
        print(f"   è¾“å…¥å½¢çŠ¶: {fake_image.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {vae_output.shape}\n")

        # æµ‹è¯• UNet
        print("[3/3] æµ‹è¯• UNet...    ", end="", flush=True)
        sys.stdout.flush()

        unet = models['unet']

        # åˆ›å»ºå‡çš„å¼ é‡
        latents = torch.randn(1, 4, 32, 32, dtype=torch.float16)
        timestep = torch.tensor([0], dtype=torch.long)
        encoder_hidden_states = torch.randn(1, 77, 4096, dtype=torch.float16)

        if torch.cuda.is_available():
            latents = latents.to("cuda")
            timestep = timestep.to("cuda")
            encoder_hidden_states = encoder_hidden_states.to("cuda")

        with torch.no_grad():
            unet_output = unet(latents, timestep, encoder_hidden_states).sample

        print("âœ…")
        print(f"   latents å½¢çŠ¶: {latents.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {unet_output.shape}\n")

        print("=" * 70)
        print("âœ… æ‰€æœ‰æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 70 + "\n")

    except Exception as e:
        print(f"âŒ")
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {type(e).__name__}")
        print(f"   {str(e)}\n")

        import traceback
        traceback.print_exc()
        sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""

    import argparse

    parser = argparse.ArgumentParser(description="Kolors æ¨¡å‹åŠ è½½æ¡†æ¶")
    parser.add_argument(
        "--quantize_mode",
        type=str,
        default="fp16",
        choices=["fp16", "4bit"],
        help="é‡åŒ–æ¨¡å¼: fp16 æˆ– 4bit"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="åŠ è½½åæ˜¯å¦è¿›è¡Œæ¨¡å‹æµ‹è¯•"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        choices=["cuda", "cpu"],
        help="è®¾å¤‡: cuda æˆ– cpu"
    )

    args = parser.parse_args()

    # åŠ è½½æ¨¡å‹
    models = load_models(quantize_mode=args.quantize_mode, device=args.device)

    # å¯é€‰ï¼šæµ‹è¯•æ¨¡å‹
    if args.test:
        test_models(models)

    print("âœ… æ¡†æ¶æ„å»ºå®Œæˆï¼æ¨¡å‹å·²å‡†å¤‡å¥½è¿›è¡Œä¸‹ä¸€æ­¥æ“ä½œ\n")

    return models


if __name__ == '__main__':
    main()